mode: fp16_multi_pretrain
model: TransEnc

data_path: data/Amazon
pretrain_file: amazon_laptops_preprocess_pretrain.pkl
#model_path: results/Amazon
model_path: results/YELP_Amazon
num_workers_per_loader: 2

seed: 42
batch_size: 32
dropout: 0.1
epoch: 80
report_frequency: 100
save_frequency: 200

head_size: 6
layers: 6
hidden_size: 300
feedforward: 1200
dense_hidden_size: 100

decoder_heads: 6
decoder_layers: 6
decoder_hidden: 300
decoder_ff: 1200
decoder_dropout: 0.2
share_emb: True

optimizer: adamw
lambda_scl: 2.0
lambda_map: 1.0
lambda_rr: 1.0
learning_rate: 0.001
learning_rate_bert: 0.001
weight_decay: 0.001
warm_up: 40000
grad_norm: 5.0

ckpt: '/Users/jordanharris/SCAPT-ABSA/checkpoints/YELP_Amazon_TransEnc/last.ckpt'
state_dict: '/Users/jordanharris/SCAPT-ABSA/results/YELP/TransEnc_0625060312/epoch_5_step_7250.pt'
dec_state_dict: '/Users/jordanharris/SCAPT-ABSA/results/YELP/TransEnc_0625060312/epoch_5_step_7250_decoder.pt'
gen_state_dict: '/Users/jordanharris/SCAPT-ABSA/results/YELP/TransEnc_0625060312/epoch_5_step_7250_generator.pt'